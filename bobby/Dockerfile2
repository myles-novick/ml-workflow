FROM centos:7
MAINTAINER abaveja@splicemachine.com

ENV SPLICEMACHINE_PY_VERSION='6.0'
ENV DIND_COMMIT='52379fa76dee07ca038624d639d9e14f4fb719ff'

# Defaults based on root folder's pom
ARG spark_version=spark-2.2.2-bin-hadoop2.6
# skipping because uber jar
#ARG scala_version=scala-2.11
ARG sm_version=2.7.0.1828
ARG cdh_version=5.14.0
ARG zookeeper_version=zookeeper-3.4.10
ARG splice_machine_version=SPLICEMACHINE-$sm_version.cdh$cdh_version.p0.dbaas
ARG hbase_version=hbase-1.2.0-cdh$cdh_version
ARG mesos_proto_jar_version=mesos-1.5.0-shaded-protobuf
ARG lib_mesos_bundle=libmesos-bundle-1.11.3
ARG zeppelin_version=splice-zeppelin-0.7.3-bin-all
ARG plsql_version=splice-compilerworks-1.1.1-SNAPSHOT
ARG hadoop_version=hadoop-2.6.0-cdh$cdh_version
ARG kafka_version=kafka_2.11-0.10.0.1
ARG hadoop_influxdb_version=hadoop-metrics-influxdb-2.6.0
ARG cdh_native_libs_version=cdh$cdh_version-native
ARG shiro_core_version=shiro-core-1.2.3
ARG shiro_web_version=shiro-web-1.2.3
ARG splice_shiro_version=splice-shiro-2.7.0.1828
ARG build_version=2.7.0.1828-1.0
ARG build_number=1533
ARG bucket_name=splicemachine
ARG splicemachine_bucket_name=$bucket_name
ARG build_folder=$build_version.$build_number/artifacts
ARG splicemachine_build_folder=$build_folder

# URI's
ENV STATIC_BASE_ARTIFACT_URI=https://s3.amazonaws.com/$bucket_name/artifacts
ENV BASE_ARTIFACT_URI=https://s3.amazonaws.com/$bucket_name/$build_folder
# skipping because uber jar
#ENV SPARK_SCALA_URI=$STATIC_BASE_ARTIFACT_URI/spark-${scala_version}.tgz
ENV HBASE_URI=$BASE_ARTIFACT_URI/$hbase_version.tar.gz
ENV SPLICEMACHINE_URI=https://s3.amazonaws.com/$splicemachine_bucket_name/$splicemachine_build_folder/$splice_machine_version.tar.gz
ENV JSCP_URI=$STATIC_BASE_ARTIFACT_URI/jscp.tgz
ENV ZOOKEEPER_URI=http://www.apache.org/dist/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gz
ENV HADOOP_URI=$BASE_ARTIFACT_URI/$hadoop_version.tar.gz
ENV KAFKA_URI=$BASE_ARTIFACT_URI/$kafka_version.tgz
ENV HADOOP_INFLUXDB_URI=$BASE_ARTIFACT_URI/$hadoop_influxdb_version.jar
ENV CDH_NATIVE_LIBS_URI=$BASE_ARTIFACT_URI/$cdh_native_libs_version.tgz
ENV ZEPPELIN_URI=$BASE_ARTIFACT_URI/$zeppelin_version.tgz
ENV PLSQL_URI=$BASE_ARTIFACT_URI/$plsql_version.tgz
ENV LIB_MESOS_URI=$BASE_ARTIFACT_URI/$lib_mesos_bundle.tar.gz
ENV MESOS_PROTO_JAR_URI=$BASE_ARTIFACT_URI/$mesos_proto_jar_version.jar
ENV SHIRO_CORE_URI=$STATIC_BASE_ARTIFACT_URI/$shiro_core_version.jar
ENV SHIRO_WEB_URI=$STATIC_BASE_ARTIFACT_URI/$shiro_web_version.jar
ENV SPLICE_SHIRO_URI=$STATIC_BASE_ARTIFACT_URI/$splice_shiro_version.jar

# Single yum layer, all shared tools, ends with cleanup
RUN \
  rpm --import https://packages.microsoft.com/keys/microsoft.asc && \
  sh -c 'echo -e "[azure-cli]\nname=Azure CLI\nbaseurl=https://packages.microsoft.com/yumrepos/azure-cli\nenabled=1\ngpgcheck=1\ngpgkey=https://packages.microsoft.com/keys/microsoft.asc" > /etc/yum.repos.d/azure-cli.repo' && \
  yum install -y epel-release && \
  yum makecache && \
  yum install -y wget curl bind-utils gawk sed xfsprogs-extra jq unzip net-tools nc which bc azure-cli && \
  curl -kLsv "https://s3.amazonaws.com/aws-cli/awscli-bundle.zip" -o "awscli-bundle.zip" && \
  unzip awscli-bundle.zip && \
  ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws && \
  rm -rf ./awscli-bundle ./awscli-bundle.zip && \
  rm -rf /var/cache/yum && yum clean all

# Single Splice users layer
RUN \
 groupadd splicemachine && \
 useradd -N -g splicemachine hbase && \
 useradd -N -g splicemachine hadoop && \
 useradd -N -g splicemachine zookeeper && \
 useradd -N -g splicemachine service_user && \
 useradd -N -g splicemachine kafka && \
 useradd -N -g splicemachine plsql && \
 useradd -N -g splicemachine zeppelin && \
 useradd -N -g splicemachine jscp && \
 groupadd hdfs && \
 groupadd hadoop && \
 useradd -N -g splicemachine hdfs && \
 usermod -a -G hdfs,hadoop hdfs

# Install customized libraries
WORKDIR /

# Zookeeper
RUN \
  curl -kLsv $ZOOKEEPER_URI | tar -xz && \
  ln -s /$zookeeper_version /zookeeper && \
  chown -h -R -L zookeeper:splicemachine /zookeeper /$zookeeper_version

# Kafka
RUN \
  curl -kLsv $KAFKA_URI | tar -xz && \
  ln -s /$kafka_version /kafka && \
  chown -h -R -L kafka:splicemachine /kafka /$kafka_version

# Plsql
RUN \
  curl -kLsv $PLSQL_URI | tar -xz && \
  chown -h -R -L plsql:splicemachine /compilerworks

# Hbase setup
RUN \
  curl -kLsv $HBASE_URI | tar -xz && \
  ln -s /$hbase_version /hbase && \
  curl -kLsv $HADOOP_INFLUXDB_URI -o /$hbase_version/lib/$hadoop_influxdb_version.jar && \
  curl -kLsv $MESOS_PROTO_JAR_URI -o /$hbase_version/lib/$mesos_proto_jar_version.jar && \
  curl -kLsv $SHIRO_CORE_URI      -o /$hbase_version/lib/$shiro_core_version.jar && \
  curl -kLsv $SHIRO_WEB_URI       -o /$hbase_version/lib/$shiro_web_version.jar && \
  curl -kLsv $SPLICE_SHIRO_URI    -o /$hbase_version/lib/$splice_shiro_version.jar && \
  mkdir -p /hbase/logs | true && \
  mkdir -p /hbase/lib/native/Linux-amd64-64 | true && \
  curl -kLsv $CDH_NATIVE_LIBS_URI | tar -xz && \
  cp -r /native/* /hbase/lib/native/Linux-amd64-64/ && \
  curl -kLsv $LIB_MESOS_URI | tar -xz && \
  cp -r /libmesos-bundle/lib/* /native/ && \
  mv /libmesos-bundle/lib/* /hbase/lib/native/Linux-amd64-64/ && \
  rm /hbase/lib/servlet-api-2.5*.jar && \
  chown -h -R -L hbase:splicemachine /hbase /$hbase_version && \
  curl -kLs $HADOOP_URI | tar -xz && \
  ln -s /$hadoop_version /hadoop && \
  cp -r /native/* /hadoop/lib/native/ && \
  chown -h -R -L hadoop:splicemachine /hadoop /$hadoop_version && \
  mkdir -p /var/lib/hadoop-hdfs && \
  chown hadoop:splicemachine /var/lib/hadoop-hdfs

# spark temp and spark config staging
RUN \
  mkdir /spark && \
  mkdir -p /$spark_version/conf && \
# skipping because uber jar
#  curl -kLs  $SPARK_SCALA_URI | tar -xz && \
#  chown -h -R -L hbase:splicemachine /$scala_version && \
  chown -h -R hbase:splicemachine /spark /$spark_version

# JSCP profiling tool
RUN \
  curl -kLs $JSCP_URI | tar -xz && \
  chown -h -R -L jscp:splicemachine /tools

# Zeppelin
RUN \
  curl -kLs $ZEPPELIN_URI | tar -xz && \
  ln -s /${zeppelin_version/splice-/} /zeppelin && \
  chown -h -R -L zeppelin:splicemachine /zeppelin /${zeppelin_version/splice-/}

# Splicemachine
RUN \
  curl -kLs $SPLICEMACHINE_URI | tar -xz && \
  ln -s /${splice_machine_version} /splicemachine && \
  chown -h -R -L hbase:splicemachine /splicemachine /${splice_machine_version}
# install yum packages and fix default encoding so click doesn't throw a fit

RUN yum clean all && \
    rm -rf /var/cache/yum && \
    yum -y install epel-release curl && \
    yum -y install git openssl openssl-devel unzip rsync lsof python java-1.8.0-openjdk-devel \
    python-pip python-devel wget gcc gcc-c++ which bind-utils net-tools which nc jq unzip && \
    yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo && \
    yum -y install docker-ce


# install python dependencies
COPY utilities/requirements.txt /tmp/requirements.txt
COPY ./target/splicemachine-jars/* $SPARK_HOME/jars/

# Install Python Splice Machine Package
RUN pip install -r /tmp/requirements.txt && \
    wget https://github.com/splicemachine/pysplice/archive/$SPLICEMACHINE_PY_VERSION.zip && \
    unzip $SPLICEMACHINE_PY_VERSION.zip && \
    cd pysplice-$SPLICEMACHINE_PY_VERSION && \
    pip install .


VOLUME /var/lib/docker
EXPOSE 2375

RUN wget -O /usr/local/bin/dind "https://raw.githubusercontent.com/docker/docker/${DIND_COMMIT}/hack/dind" && \
	chmod a+x /usr/local/bin/dind

#RUN rm -rf /usr/local/spark-2.1.1-bin-hadoop2.6/jars/slf4j-log4j12-1.7.10.jar

# Copy files
RUN mkdir -p /bob
COPY . /bob
# make the entrypoint executable by all users
RUN chmod a+x /bob/utilities/entrypoint.sh && \
    chmod a+x /bob/utilities/run_dind.sh && \
    cat /bob/goodies/bobby.txt

# do good stuff
CMD ["/bob/utilities/entrypoint.sh"]
